{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import Imputer as IMP\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder as LBLENC\n",
    "from sklearn.preprocessing import StandardScaler as SSCL\n",
    "from sklearn.preprocessing import MinMaxScaler as MMSCL\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "from sklearn.linear_model import LogisticRegression as LREG\n",
    "from sklearn import svm as SVM\n",
    "from sklearn.metrics import classification_report as CREP\n",
    "from sklearn.metrics import confusion_matrix as CMAT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some Constants\n",
    "\"\"\"\n",
    "\n",
    "PLOTS = 'F:/Path/of your/Data/'\n",
    "\n",
    "cols1 = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig',\n",
    "         'breast', 'breast-quad', 'irradiate']\n",
    "cols2 = ['ID', 'Clump_Thickness', 'Uniformity_of_Cell_Size', 'Uniformity_of_Cell_Shape',\n",
    "         'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei', 'Bland_Chromatin',\n",
    "         'Normal_Nucleoli', 'Mitoses', 'Class']\n",
    "cols3 = ['ID', 'Class', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function creates DataFrame of Dataset\n",
    "\"\"\"\n",
    "\n",
    "def OnCreateDF( fname, sep = ',', cols = None, index = None ):\n",
    "    \n",
    "    data = pd.read_csv( fname, sep = sep, header = None, names = cols, index_col = index )\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following are functions dedicated for visualization.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "OnPlotCat function is used to visualize categorical features. The function plots Bar graph of a single \n",
    "categorical feature and shows the frequency of the feature within each class of the dependent variable.\n",
    "\n",
    "data = The DataFrame.\n",
    "col1 = Independent Categorical Feature\n",
    "col2 = Dependent Variable\n",
    "\n",
    "\"\"\"\n",
    "def OnPlotCat( data, col1, col2, sav = False, name = 'out' ):\n",
    "    \n",
    "    datav = data.groupby( col2 )[col1].value_counts().unstack(0)\n",
    "    ax = datav.plot.barh()\n",
    "    if sav:\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig( PLOTS + name + '.png', bbox_inches = 'tight', dpi = 1000 )\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnPlotBox is used to create a Box Plot. The function creates a box plot of multiple numeric variables.\n",
    "\n",
    "data = The DataFrame.\n",
    "cols = List of columns names that will be represented by the plot.\n",
    "\n",
    "\"\"\"\n",
    "def OnPlotBox( data, cols, sav = False, name = 'out2' ):\n",
    "    \n",
    "    vect = []\n",
    "    for c in data.columns:\n",
    "        if c not in ['Class']:\n",
    "            vect.append( data[c] )\n",
    "    \n",
    "    plt.boxplot( vect, patch_artist = True )\n",
    "    plt.xticks( list(range(1, len(cols)+1)), cols )\n",
    "    if sav:\n",
    "        plt.savefig( PLOTS + name + '.png', bbox_inches = 'tight', dpi = 1000 )\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnPlotBox is used to create a Box Plot. However, unlike OnPlotBox(), this function creates a BoxPlot\n",
    "with classes of the dependent variable in the X-Axis allowing us to understand the distribution of\n",
    "numeric variables within each category of the dependent variable.\n",
    "\n",
    "data = The DataFrame.\n",
    "cname = Name of the column of dependent variable.\n",
    "\n",
    "\"\"\"\n",
    "def OnPlotBoxClass( data, cname = 'Class' ):\n",
    "    \n",
    "    boxplot = data.boxplot( by = cname )\n",
    "    boxplot.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnViolinPlot is used to create a Violin Plot of the given data.\n",
    "\n",
    "data = The DataFrame.\n",
    "class_col = Either \"NaN\" or the dependent variable column as a List.\n",
    "\n",
    "\"\"\"\n",
    "def OnViolinPlot(data, class_col = 'NaN'):\n",
    "    \n",
    "    if class_col != 'NaN':\n",
    "        data['Class'] = class_col\n",
    "    data = pd.melt( data, id_vars = 'Class', var_name = 'features', value_name = 'value' )\n",
    "    plt.figure( figsize = (10,10) )\n",
    "    \n",
    "    sns.violinplot( x = 'features', y = 'value', hue = 'Class', data = data, split = True, inner = 'quart' )\n",
    "    plt.xticks( rotation = 90 )\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnSwarmpPlot is used to create a Swarmp plot. Swarmp Plots are helpful to understand how finely values of \n",
    "independent variables are distributed among the categories of dependent variable.\n",
    "\n",
    "data = The DataFrame.\n",
    "class_col = Either \"NaN\" or the dependent variable column as a List.\n",
    "\n",
    "\"\"\"\n",
    "def OnSwarmpPlot(data, class_col = 'NaN'):\n",
    "\n",
    "    if class_col != 'NaN':\n",
    "        data['Class'] = class_col\n",
    "    data = pd.melt( data, id_vars = 'Class', var_name = 'features', value_name = 'value' )\n",
    "    plt.figure( figsize = (10,10) )\n",
    "\n",
    "    sns.swarmplot( x = 'features', y = 'value', hue = 'Class', data = data )\n",
    "    plt.xticks( rotation = 90 )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnPlotHeatMap creates a Heat Map. The plot can be used for understanding the corelation among the \n",
    "variables that can help in Feature Selection.\n",
    "\n",
    "data = The DataFrame.\n",
    "class_col = Either \"NaN\" or the dependent variable column as a List.\n",
    "\n",
    "\"\"\"\n",
    "def OnPlotHeaMap(data):\n",
    "    \n",
    "    f, ax = plt.subplots( figsize = (18,18) )\n",
    "    sns.heatmap( data = data.corr(), annot = True, linewidths = 0.5, fmt = '.1f' )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following functions are dedicated to processing the datasets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnLabelEncode is used to Label encode Categorical Features.\n",
    "\n",
    "data = The DataFrame.\n",
    "class_col = Either \"all\" or List of columns that must be Label Encoded.\n",
    "\n",
    "\"\"\"\n",
    "def OnLabelEncode( data, cols = 'all' ):\n",
    "    \n",
    "    lblenc = LBLENC();\n",
    "\n",
    "    if cols == 'all':\n",
    "        for c in data.columns:\n",
    "            data[c] = lblenc.fit_transform( data[c] )\n",
    "        return data\n",
    "    else:\n",
    "        for i in cols:\n",
    "            data.iloc[:, i] = lblenc.fit_transform( data.iloc[:, i] )\n",
    "        return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnSetDummyVars is used to create Dummy Variables of Label Encoded Categorical Features.\n",
    "\n",
    "data = The DataFrame.\n",
    "class_col = Either \"all\" or List of columns that must be converted to Dummy Variables.\n",
    "\n",
    "\"\"\"\n",
    "def OnSetDummyVars( data, cols = 'all' ):\n",
    "    \n",
    "    if cols == 'all':\n",
    "        for c in data.columns:\n",
    "            data = pd.get_dummies( data,  prefix = [ c ], columns = [ c ] )\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        col_names = [ data.columns[i] for i in cols ]\n",
    "        for i in col_names:\n",
    "            data = pd.get_dummies( data, prefix = [ i ], columns = [ i ] )\n",
    "\n",
    "        return data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnReplaceValue function is used to handel missing attributes in the dataset either Categorical or Numeric.\n",
    "\n",
    "data = The DataFrame.\n",
    "nvalue = String that represents missing values in the dataset.\n",
    "myval = Custom value to replace the missing value with. Must be 'NaN' if not required.\n",
    "cols = Either 'all' or the List of columns that must be processed.\n",
    "dtype = Must be 'num' if given columns represent Numeric features or 'cat' if they represent\n",
    "        Categorical Features.\n",
    "strat = Represents the replacement strategy for missing values like mean, max, min, etc.\n",
    "\n",
    "\"\"\"\n",
    "def OnReplaceValue( data, nvalue = '?', myval = 'NaN', cols = 'all', dtype = 'num', strat = 'mean' ):\n",
    "    \n",
    "    if dtype == 'cat':\n",
    "        \n",
    "        if cols == 'all':\n",
    "            for c in data.columns:\n",
    "                if myval == 'NaN':\n",
    "                    data[c] = data[c].replace( nvalue, data[c].value_counts().idxmax() )\n",
    "                else:\n",
    "                    data[c] = data[c].replace( nvalue, myval )\n",
    "\n",
    "            return data\n",
    "        else:\n",
    "            for i in cols:\n",
    "                if myval == 'NaN':\n",
    "                    data.iloc[:, i] = data.iloc[:, i].replace( nvalue, data.iloc[:, i].value_counts().idxmax() )\n",
    "                else:\n",
    "                    data[c] = data[c].replace( nvalue, myval )\n",
    "\n",
    "            return data\n",
    "\n",
    "    elif dtype == 'num':\n",
    "        \n",
    "        if myval == 'NaN':\n",
    "\n",
    "            if cols == 'all':\n",
    "                \n",
    "                impu = IMP( missing_values = nvalue, strategy = strat, axis = 0 )\n",
    "                impu = impu.fit( data2.iloc[:, :] )\n",
    "                data.iloc[:, :] = impu.transform( data.iloc[:, :] )\n",
    "                \n",
    "                return data\n",
    "            else:\n",
    "\n",
    "                for i in cols:\n",
    "                    impu = IMP( missing_values = nvalue, strategy = strat, axis = 0 )\n",
    "                    impu = impu.fit( data2.iloc[:, i:i+1] )\n",
    "                    data.iloc[:, i:i+1] = impu.transform( data.iloc[:, i:i+1] )\n",
    "                    \n",
    "                return data\n",
    "        else:\n",
    "            if cols == 'all':\n",
    "                data = data.replace( nvalue, myval )\n",
    "\n",
    "                return data\n",
    "            else:\n",
    "                for i in cols:\n",
    "                    data.iloc[:, i] = data.iloc[:, i].replace( nvalue, myval )\n",
    "\n",
    "                return data\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnMinMaxScale is used to scale a DataFrame using Normalization.\n",
    "\n",
    "data = The DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "def OnMinMaxScale( data ):\n",
    "    \n",
    "    mmscl = MMSCL()\n",
    "    return pd.DataFrame( mmscl.fit_transform(data) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OnStdScale is used to scale a DataFrame using Standardisation.\n",
    "\n",
    "data = The DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "def OnStdScale( data ):\n",
    "    \n",
    "    sscl = SSCL()\n",
    "    return pd.DataFrame( sscl.fit_transform(data) )\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "OnSplitData is used to split the dataset into Train/Test sets possessing equal propotion of all the\n",
    "categories of the dependent variable.\n",
    "\n",
    "data = The DataFrame.\n",
    "t_size = Size of test set.\n",
    "\n",
    "\"\"\"\n",
    "def OnSplitData( data, t_size = 0.2 ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Assuming the dependent variable has two categories, we first create two independent dataframes each\n",
    "    containing data of only one of the categories of dependent variable.\n",
    "    \"\"\"\n",
    "    data_c1 = data.loc[ data['Class'] == data['Class'].unique()[0] ]\n",
    "    data_c2 = data.loc[ data['Class'] == data['Class'].unique()[1] ]\n",
    "\n",
    "    \"\"\"\n",
    "    Next, we train_test_split each of the previouly created datset individually.\n",
    "    \"\"\"    \n",
    "    c1_xtrain, c1_xtest, c1_ytrain, c1_ytest = tts( data_c1, data_c1['Class'], test_size = t_size, random_state = 100 )\n",
    "    c2_xtrain, c2_xtest, c2_ytrain, c2_ytest = tts( data_c2, data_c2['Class'], test_size = t_size, random_state = 100 )\n",
    "    \n",
    "    \"\"\"\n",
    "    We concat the Train Sets and Test Sets of each categories obtained from previous step to a single\n",
    "    Train and Test set.\n",
    "    \"\"\"\n",
    "    xtrain = pd.concat( [c1_xtrain, c2_xtrain] )\n",
    "    xtest = pd.concat( [c1_xtest, c2_xtest] )\n",
    "    \n",
    "    \"\"\"\n",
    "    Shuffle to maintain randomness.\n",
    "    \"\"\"\n",
    "    xtrain = shuffle(xtrain)\n",
    "    ytrain = xtrain['Class']\n",
    "    xtest = shuffle(xtest)\n",
    "    ytest = xtest['Class']\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OnRunModels is used to run Models on the dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def OnRunModels(data):\n",
    "    \n",
    "    xtrain, xtest, ytrain, ytest = OnSplitData( data )\n",
    "\n",
    "    knn = KNN( n_neighbors = 10, metric = 'minkowski', p = 2 )\n",
    "    svm = SVM.SVC( kernel = 'linear', C = 1.0, gamma = 'auto' )\n",
    "    mnb = MNB()\n",
    "    lreg = LREG( random_state = 0 )\n",
    "    \n",
    "    knn.fit( xtrain, ytrain )\n",
    "    knn_preds = knn.predict( xtest )\n",
    "    print(' KNN Results : ')\n",
    "    print(' Classification Report')\n",
    "    print( CREP( ytest, knn_preds ) )\n",
    "    print(' Confusion Matrix')\n",
    "    print( CMAT( ytest, knn_preds ), '\\n\\n' )\n",
    "    \n",
    "    svm.fit( xtrain, ytrain )\n",
    "    svm_preds = svm.predict( xtest )\n",
    "    print(' SVM Results : ')\n",
    "    print(' Classification Report')\n",
    "    print( CREP( ytest, svm_preds ) )\n",
    "    print(' Confusion Matrix')\n",
    "    print( CMAT( ytest, svm_preds ), '\\n\\n' )\n",
    "\n",
    "    mnb.fit( xtrain, ytrain )\n",
    "    mnb_preds = mnb.predict( xtest )\n",
    "    print(' MNB Results : ')\n",
    "    print(' Classification Report')\n",
    "    print( CREP( ytest, mnb_preds ) )\n",
    "    print(' Confusion Matrix')\n",
    "    print( CMAT( ytest, mnb_preds ), '\\n\\n' )\n",
    "\n",
    "    lreg.fit( xtrain, ytrain )\n",
    "    lreg_preds = lreg.predict( xtest )\n",
    "    print(' LREG Results : ')\n",
    "    print(' Classification Report')\n",
    "    print( CREP( ytest, lreg_preds ) )\n",
    "    print(' Confusion Matrix')\n",
    "    print( CMAT( ytest, lreg_preds ), '\\n\\n' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read each of the datasets into seperate dataframes.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data1 = OnCreateDF( 'F:/Python/Project 1/Data1/breast-cancer.csv', cols = cols1 )\n",
    "\n",
    "data2 = OnCreateDF( 'F:/Python/Project 1/Data2/breast-cancer-wisconsin.csv', cols = cols2, index = 'ID' )\n",
    "\n",
    "data3 = OnCreateDF( 'F:/Python/Project 1/Data3/wdbc.csv', cols = cols3, index = 'ID' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 1\n",
    "\"\"\"\n",
    "data1.head(5)\n",
    "data1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace Missing Values.\n",
    "data1 = OnReplaceValue( data1, dtype = 'cat' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do visulization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the columns except deg-malig are categorical in the dataset. Hence we use OnPlotCat.\n",
    "OnPlotCat(data1, 'age', 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use to OnBoxPlotClass to view the distribution of the values in the column within each category of\n",
    "# the dependent variable.\n",
    "OnPlotBoxClass( data1[['Class', 'deg-malig']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Beacuse deg-malig is the only numeric column in the data set and rest all are categorical we save it in a\n",
    "# temproary variable. This will make it easire to use OnLabelEncode() and OnSetDummyVars() as we will not\n",
    "# have to call the functions multiple times specifying the columns before and after 'deg-malig' which\n",
    "# would have been the case if we processed otherwise.\n",
    "\n",
    "temp_col = list( data1['deg-malig'] )\n",
    "data1.drop( 'deg-malig', inplace = True, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Label Encode and Dummy Encode the categorical features. The second parameter of the OnDummyEncode()\n",
    "# is a List of indices of the columns that must be Dummified starts with 1 to ensure that the dependent\n",
    "# variable ie. \"Class\" is not dummified.\n",
    "\n",
    "data1 = OnLabelEncode( data1 )\n",
    "data1 = OnSetDummyVars( data1, list( range( 1, len(data1.columns) ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recreate the deg-malig column.\n",
    "data1['deg-malig'] = temp_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run models.\n",
    "OnRunModels(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 2\n",
    "\"\"\"\n",
    "data2.head(5)\n",
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values. Notice that we replace the missing values that are represented by \"?\" by a custom\n",
    "# number NaN. This is beacuse firstly all the indenpendent variable columns in the datset are numeric.\n",
    "# Second, all the columns are ranged between 1 - 10.\n",
    "# Third, because the missing value is represented by \"?\" the columns containing missing values are\n",
    "# of type String. This prevents us from using standard missing value replacement strategies like mean, max.\n",
    "# Hence we first replace the missing values with NaN which is numberic.\n",
    "# Next we convert the columns to type numeric. Next we replace them using standard methods.\n",
    "\n",
    "data2 = OnReplaceValue( data2, myval = np.nan )\n",
    "data2[data2.columns[5]] = data2[data2.columns[5]].apply(pd.to_numeric)\n",
    "data2 = OnReplaceValue( data2, nvalue = np.nan, cols = [5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do Visulization.\n",
    "OnPlotBox(data2, data2.columns[:9])\n",
    "OnPlotBoxClass( data2 )\n",
    "OnViolinPlot(data2)\n",
    "OnSwarmpPlot(data2)\n",
    "OnPlotHeaMap(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From above visulization especially Violin and Swarmp Plot, it can be seen that Mitoses is not very\n",
    "# cleanly distributed. Hencce, we can drop if required.\n",
    "data2.drop( 'Mitoses', inplace = True, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Models.\n",
    "OnRunModels(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 2\n",
    "\"\"\"\n",
    "\n",
    "data3.head(5)\n",
    "data3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the vaues of the Class ( Diagnois ) column with numeric data. Not required though.\n",
    "data3['Class'] = data3['Class'].replace( 'B', 2 )\n",
    "data3['Class'] = data3['Class'].replace( 'M', 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the dependent variable column in a temproary variable for later use and drop it from the dataframe.\n",
    "# This will help us in using the plotting functions defined above.\n",
    "data3_class = list( data3['Class'] )\n",
    "data3.drop( 'Class', inplace = True, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a feature scaling function.\n",
    "data3 = OnMinMaxScale( data3 )\n",
    "data3.columns = cols3[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do Visualization.\n",
    "# Because there are 30 columns in the dataset and we know that they are just variations the actual 10\n",
    "# data, in groups of 10 we plot the dataset in groups of 10. This makes it easy to visualize.\n",
    "\n",
    "\"\"\"\n",
    "Box Plots\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotBoxClass( pd.concat( [data3.iloc[:, 0:10], pd.Series(data3_class)], axis = 1 ), cname = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotBoxClass( pd.concat( [data3.iloc[:, 10:20], pd.Series(data3_class)], axis = 1 ), cname = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotBoxClass( pd.concat( [data3.iloc[:, 20:], pd.Series(data3_class)], axis = 1 ), cname = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HeatMaps\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotHeaMap( pd.concat( [data3, pd.Series(data3_class)], axis = 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotHeaMap(data3.iloc[:, 0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotHeaMap(data3.iloc[:, 10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotHeaMap(data3.iloc[:, 20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BoxPlot\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotBox(data3.iloc[:, 0:10], cols = data3.columns[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotBox(data3.iloc[:, 10:20], cols = data3.columns[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnPlotBox(data3.iloc[:, 20:], cols = data3.columns[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Violin Plot\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnViolinPlot(data3.iloc[:, 0:10], data3_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnViolinPlot(data3.iloc[:, 10:20], data3_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnViolinPlot(data3.iloc[:, 20:], data3_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Swarmp Plot\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnSwarmpPlot(data3.iloc[:, 0:10], data3_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnSwarmpPlot(data3.iloc[:, 10:20], data3_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnSwarmpPlot(data3.iloc[:, 20:], data3_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# drop_cols contains the names of the columns that will be droped from the dataset based on what we have\n",
    "# conculded from the visualization.\n",
    "\n",
    "# Beacuse there are 30 columns which are in 3 groups of 10 similar data just mathematical variations\n",
    "# ( mean, standard error and worst ) we know that strong correlation must exist between these features.\n",
    "# And the best way to understand these correlations is using the good old HeatMap. We try to eliminate\n",
    "# columns with high correlations. For example, from the heat map we can see that 'area_mean',\n",
    "# 'radius_mean' and 'perimeter_mean' are correlated. We drop 'radius_mean' and 'perimeter_mean' and go\n",
    "# with area_mean. The reason for going with area_mean can be understood by viewing the Voilin Plot and\n",
    "# Swarmp Plot of the mean ( first 10 ) features of the dataset. We see that area_mean is musch better\n",
    "# distributed as compated to the other two. Hence we go with area_mean. \n",
    "# The same procedure goes for selecting a feature from other correlated ones like 'compactness_mean',\n",
    "# 'concave points_mean' and 'concavity_mean' where we go with 'concavity_mean', and so on.\n",
    "\n",
    "# Based on the visualization several combinations of features can be come up with. We decide to go with\n",
    "# the below configuration.\n",
    "\n",
    "drop_cols = ['radius_mean', 'perimeter_mean', 'compactness_mean', 'concave points_mean',\n",
    "             'radius_se', 'perimeter_se', 'compactness_se', 'concave points_se',\n",
    "             'radius_worst', 'area_worst', 'perimeter_worst', 'texture_worst',\n",
    "             'compactness_worst', 'concave points_worst']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop selected columns.\n",
    "\n",
    "data3_new = data3.copy(deep = False)\n",
    "data3_new.drop( drop_cols, inplace = True, axis = 1 )\n",
    "\n",
    "OnPlotHeaMap( pd.concat( [data3_new, pd.Series(data3_class)], axis = 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnSwarmpPlot(data3_new.iloc[:, 0:6], data3_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnSwarmpPlot(data3_new.iloc[:, 6:11], data3_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnSwarmpPlot(data3_new.iloc[:, 12:], data3_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Model.\n",
    "data3_new['Class'] = data3_class\n",
    "OnRunModels(data3_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Merging of datasets on Patient ID does not seem possible. This is because the first dataset lacks the\n",
    "information.\n",
    "While dataset 2 and 3 possess the Patient ID column there is not a single entry in the column of the 2 \n",
    "dataset that overlap/are similar, every single Patient ID in both the set is unique. Hence merging\n",
    "the dataset would simply mean concating the two frames and for every Patient ID of one dataset \n",
    "nullifying the values of all the columns of the other dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
